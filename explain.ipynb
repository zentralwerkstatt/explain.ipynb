{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating feature visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this notebook as a REST API\n",
    "\n",
    "This notebook can be run in the usual Jupyter way, by executing cells subsequently. To facilitate the exploration of the space of visualizations for non-experts, however, this notebook also provides a REST API through the [`jupyter-kernelgateway`](https://github.com/jupyter/kernel_gateway) package. The `jupyter-kernelgateway` package implements a simple way to turn notebooks into APIs, avoiding the many incomptabilities of Jupyter and, for instance, Flask. This way, the literal style of the code can be kept, while also providing a way to run it as a microservice without any modifications.\n",
    "\n",
    "### Manual installation\n",
    "\n",
    "In a terminal, change into the directory of this notebook and run:\n",
    "\n",
    "`\n",
    "jupyter kernelgateway --KernelGatewayApp.api='kernel_gateway.notebook_http' --KernelGatewayApp.seed_uri='backend.ipynb' --port=6006 --KernelGatewayApp.allow_origin=*\n",
    "`\n",
    "\n",
    "The port 6006 in this example is arbitrary and can be switched for any open port. The API can now be accesses from the local host. To make it accessible from the network, add the `--ip=*` parameter. Only use this setting if you know what you are doing. The `--KernelGatewayApp.allow_origin=*` parameter makes sure that the API send CORS headers so the generated images can be accessed by the frontend.\n",
    "\n",
    "### Installation using docker\n",
    "\n",
    "Install Docker and run either `run_GPU.sh` or `run_CPU.sh`. The notebook server should be exposed on port 8888, the API should be exposed on port 6006 (this can be changed in the Dockerfile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The default settings have shown to provide good results for both the InceptionV3 and the VGG16 architectures.\n",
    "\n",
    "- `arch_or_file`: Set to either a valid Keras model file location, or to a valid implemented Keras model architecture. The latter setting will load this architecture with its respective ImageNet weights. Currentyl implemented architectures are `InceptionV3` and `VGG16`.\n",
    "- `iteration`: How many iterations of gradient ascent to do per octave.\n",
    "- `step`: Step size\n",
    "- `median_every`: Regularization term. How often to apply a [median filter](https://en.wikipedia.org/wiki/Median_filter) to the image during gradient ascent (every `median_every` iterations).\n",
    "- `median_fsize`: Regularization term. Size of the median filter kernel.\n",
    "- `octaves`: How many octaves, i.e. how many different scales of the image, to process.\n",
    "- `octaves_scale`: How big is the increase in image size between octaves.\n",
    "- `final_size`: Size of the image after the last octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These settings are always used\n",
    "arch_or_file = 'InceptionV3'\n",
    "iterations = 100\n",
    "step = 0.01\n",
    "median_every = 4\n",
    "median_fsize = 5\n",
    "octaves = 10\n",
    "octave_scale = 1.2\n",
    "final_size = 900\n",
    "\n",
    "# These are only used when running standalone\n",
    "layer = 'predictions'\n",
    "neuron = 100\n",
    "octave = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please refer to the [README.md](README.md) for proper installation of the dependencies listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage.filters import median_filter\n",
    "from scipy.ndimage import zoom\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from queue import PriorityQueue\n",
    "from threading import Thread\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image helper functions\n",
    "\n",
    "These functions are part of all notebooks in the repository. They provide some simple image transformations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizes an image array from the -1/1 value space to the 0/255 value space.\n",
    "def deprocess_img(img, autotone=True):\n",
    "    img /= 2.\n",
    "    img += 0.5\n",
    "    img *= 255.\n",
    "    img = np.clip(img, 0, 255).astype('uint8') # Clip to visible range\n",
    "    if (autotone): img = autotone_img(img)\n",
    "    return img\n",
    "\n",
    "# Saves a normalized image array as a regular image file.\n",
    "def save_img(img, filename, fmt='jpeg'): \n",
    "    PIL.Image.fromarray(img).save(filename, fmt)\n",
    "\n",
    "# Resizes an imge array. Supports n-dimensional batches.\n",
    "def resize_img(img, size):\n",
    "    factors = np.ones(shape=len(img.shape), dtype=np.float)\n",
    "    factors[-2] = float(size[1]) / img.shape[-3]\n",
    "    factors[-3] = float(size[0]) / img.shape[-2]\n",
    "    img = np.copy(img)\n",
    "    factors = tuple(factors.tolist())\n",
    "    return zoom(img, factors, order=1)\n",
    "\n",
    "# Applies a median filter to an image array. Supports n-dimensional batches.\n",
    "def median_filter_img(img, fsize):\n",
    "    factors = np.ones(shape=len(img.shape), dtype=np.int)\n",
    "    factors[-2] = fsize\n",
    "    factors[-3] = fsize\n",
    "    factors = tuple(factors.tolist())\n",
    "    return median_filter(img, size=factors)\n",
    "\n",
    "# Shows a normalized image array in the cell output.\n",
    "def show_img(img, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(img).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "    \n",
    "# Normalizes each color of an image array sepearately (\"Photoshop auto tone\").\n",
    "def autotone_img(img):\n",
    "    img[:,:,0] = np.interp(img[:,:,0], [np.amin(img[:,:,0]), np.amax(img[:,:,0])], [0, 255])\n",
    "    img[:,:,1] = np.interp(img[:,:,1], [np.amin(img[:,:,1]), np.amax(img[:,:,1])], [0, 255])\n",
    "    img[:,:,2] = np.interp(img[:,:,2], [np.amin(img[:,:,2]), np.amax(img[:,:,2])], [0, 255])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main activation loop\n",
    "\n",
    "The main operations to generate activations are contained in a single function so they can be run in a separate thread (important if the notebook is run in API mode). Please refer to the inline commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activation(q):\n",
    "    \n",
    "    # Load model inside thread to avoid confusing Keras\n",
    "    model = None\n",
    "    # Is this a valid architecture?\n",
    "    if (arch_or_file == 'InceptionV3'): model = InceptionV3(weights='imagenet', include_top=True)\n",
    "    elif (arch_or_file  == 'VGG16'): model = VGG16(weights='imagenet', include_top=True)\n",
    "    else:\n",
    "        # Is this a valid Keras model file?\n",
    "        try: model = load_model(arch_or_file)\n",
    "        except: \n",
    "            raise FileNotFoundError('First parameter is not valid Keras architecture or valid Keras model file.')\n",
    "            return\n",
    "    model_list = [layer.name.lower() for layer in model.layers]\n",
    "    model_dict = dict([(layer.name.lower(), layer) for layer in model.layers])\n",
    "    model_json = dict([(layer.name.lower(), layer.output_shape[-1]) for layer in model.layers])\n",
    "        \n",
    "    # Persistent graph edits\n",
    "    loss = K.variable(0.)\n",
    "    input_layer = model.layers[0].input \n",
    "        \n",
    "    while True:\n",
    "\n",
    "        layer, neuron, variations, octave, path_to_img = q.get()[1] # Ignore the priority item\n",
    "        \n",
    "        # If there are no jobs to do, loop\n",
    "        if neuron is None:\n",
    "            break\n",
    "        \n",
    "        # If layer is invalid, scrap this job and loop\n",
    "        if layer not in model_list:\n",
    "            raise KeyError('Layer ' + layer + ' is not part of model ' + arch_or_file + '.')\n",
    "            q.task_done()\n",
    "            break\n",
    "\n",
    "        # Is this a classification layer?\n",
    "        predictions = False\n",
    "        if (layer.lower() == 'predictions'):\n",
    "            predictions = True\n",
    "\n",
    "        target_layer = model_dict[layer].output\n",
    "        # We stack an additional dimension to generate activations in batches\n",
    "        # We avoid border artifacts by only involving non-border pixels in the loss\n",
    "        if (predictions): loss = K.stack([model.layers[-1].output[i, neuron] for i in range(variations)])\n",
    "        else: loss = K.stack([K.sum(K.mean(target_layer[i, 2:-2, 2:-2, neuron])) for i in range(variations)])  \n",
    "        grads = K.gradients(loss, input_layer)[0] # Compute gradients from loss\n",
    "        grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon()) # Normalize\n",
    "        fetch_loss_and_grads = K.function([input_layer], [loss, grads]) # As Keras function\n",
    "\n",
    "        # Input noise image\n",
    "        img = np.random.normal(0, 0.01, (variations, final_size, final_size, 3))\n",
    "\n",
    "        # Explicitly copy octave list and cut to only generate up to desired octave for this neuron\n",
    "        successive_shapes = list(all_successive_shapes[:octave])\n",
    "\n",
    "        # Excplicitly copy noise image\n",
    "        final_img = np.copy(img)\n",
    "        \n",
    "        # Do octaves\n",
    "        shrunk_final_img = resize_img(img, successive_shapes[0])\n",
    "        for shape in successive_shapes:\n",
    "\n",
    "            # Resize current image to current shape\n",
    "            img = resize_img(img, shape) \n",
    "\n",
    "            # Optimize current image w.r.t. selected layers with gradient ascent\n",
    "            for i in range(iterations):\n",
    "\n",
    "                # Get losses and gradients\n",
    "                grad_values = fetch_loss_and_grads([img])[1]\n",
    "\n",
    "                # Gradient ascent\n",
    "                img += step * grad_values\n",
    "\n",
    "                # Regularization\n",
    "                # No regularization on last iteration for good quality output  \n",
    "                if (i != iterations - 1):  \n",
    "                    if (median_fsize is not 0 and i % median_every == 0) :\n",
    "                        img = median_filter_img(img, median_fsize)\n",
    "                \n",
    "                # Save every iteration to show progress in interface\n",
    "                deprocessed_img = deprocess_img(np.copy(img[0]))\n",
    "                save_img(deprocessed_img, path_to_img)\n",
    "\n",
    "            # Resize (\"upscale\") previous shape original image to current shape\n",
    "            upscaled_shrunk_final_img = resize_img(shrunk_final_img, shape)\n",
    "\n",
    "            # Resize (\"downscale\") original shape original image to current shape\n",
    "            same_size_final_img = resize_img(final_img, shape)\n",
    "\n",
    "            # Find the details that are lost in the upscaling process\n",
    "            lost_detail = same_size_final_img - upscaled_shrunk_final_img\n",
    "\n",
    "            # Add these details back to the optimized image\n",
    "            img += lost_detail # Add back the lost details to the optimized image\n",
    "\n",
    "            # Prepare for next ocatve\n",
    "            shrunk_final_img = resize_img(final_img, shape)\n",
    "\n",
    "        # Show images\n",
    "        # for v in range(img.shape[0]):\n",
    "            # deprocessed_img = deprocess_img(np.copy(img[v]))\n",
    "            # show_image(deprocessed_img) \n",
    "            \n",
    "        # Save image and remove from queue\n",
    "        deprocessed_img = deprocess_img(np.copy(img[0]))\n",
    "        save_img(deprocessed_img, path_to_img)\n",
    "        q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare priority queue and thread\n",
    "if not os.path.exists('images'): os.makedirs('images')\n",
    "q = PriorityQueue()\n",
    "t = Thread(target=generate_activation, args=(q,))\n",
    "t.start()\n",
    "\n",
    "# Prepare octaves, they are the same for all neurons/layers\n",
    "final_shape = (final_size, final_size)\n",
    "all_successive_shapes = [final_shape]\n",
    "for i in range(1, octaves):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in final_shape])\n",
    "    all_successive_shapes.append(shape)\n",
    "# Reverse the list so we go small to final\n",
    "all_successive_shapes = all_successive_shapes[::-1]\n",
    "\n",
    "# Prepare placeholders for all octaves\n",
    "placeholders = []\n",
    "for n, shape in enumerate(all_successive_shapes):\n",
    "    wh = all_successive_shapes[n][0] # Images are always square\n",
    "    img = np.random.normal(0, 0.01, (wh, wh, 3))\n",
    "    img = deprocess_img(img)\n",
    "    placeholders.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST resource/main loop\n",
    "\n",
    "`jupyter kernelgateway` routes API requests according to the first comment line. If we are not in API mode, the global notebook settings are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/712811ca-4426-45c3-8790-7cbcddec7223.jpg\n"
     ]
    }
   ],
   "source": [
    "# GET /activations/:layer/:neuron/:octave\n",
    "\n",
    "variations = 1\n",
    "# Check if we are running in API mode\n",
    "if 'KERNEL_GATEWAY' in os.environ:\n",
    "    # Extract parameters from path\n",
    "    r = json.loads(REQUEST)\n",
    "    layer = r['path']['layer']\n",
    "    neuron = int(r['path']['neuron'])\n",
    "    variations = 1\n",
    "    octave = int(r['path']['octave'])\n",
    "\n",
    "# Generate unique image ID\n",
    "path_to_img = 'images/' + str(uuid.uuid4()) + '.jpg'\n",
    "# Write placeholder image\n",
    "save_img(placeholders[octave-1], path_to_img)\n",
    "# Return image path              \n",
    "print(path_to_img)\n",
    "# Start generating activation\n",
    "q.put((11-octave, (layer, neuron, variations, octave, path_to_img))) # 11 So priority is at least 1!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
